{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Boosting Classifiers Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima Diabetes Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Datasets/pima-indians-diabetes.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['preg', 'glu', 'bp', 'sft', 'ins', 'bmi', 'dpf', 'age', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('outcome', axis=1)\n",
    "outcome = df['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Parkinson's Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parkinson = pd.read_csv('Datasets/parkinson.csv')\n",
    "# parkinson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = parkinson.drop(['name', 'status'], axis=1)\n",
    "# outcome = parkinson['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Splitting Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcome, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing Performace of fully grown DT, Bagged DT, Random Forests, Bagged NB and Bagged KNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Grown DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__max_features': None, 'base_estimator__max_leaf_nodes': None, 'base_estimator__min_impurity_decrease': 0.0, 'base_estimator__min_impurity_split': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'base_estimator__min_weight_fraction_leaf': 0.0, 'base_estimator__presort': False, 'base_estimator__random_state': 2, 'base_estimator__splitter': 'best', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
      "            splitter='best'), 'learning_rate': 1.0, 'n_estimators': 1, 'random_state': 2}\n"
     ]
    }
   ],
   "source": [
    "def fit_dt_adaboost(X, y):\n",
    "    model = AdaBoostClassifier(base_estimator=dt, random_state=2)\n",
    "    params = {'n_estimators':list(np.arange(1, 101))}\n",
    "    grid = GridSearchCV(model, params, scoring='recall', cv=3)\n",
    "    grid_fit = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_fit.best_estimator_\n",
    "\n",
    "model_dt = fit_dt_adaboost(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'n_neighbors'\n",
    "print(model_dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_dt = AdaBoostClassifier(base_estimator=dt, random_state=2, n_estimators=51).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 97, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': 2, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "def fit_gbdt(X, y):\n",
    "    model = GradientBoostingClassifier(random_state=2)\n",
    "    params = {'n_estimators':list(np.arange(1, 101))}\n",
    "    grid = GridSearchCV(model, params, scoring='recall', cv=3)\n",
    "    grid_fit = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_fit.best_estimator_\n",
    "\n",
    "model_gbdt = fit_gbdt(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'n_neighbors'\n",
    "print(model_gbdt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(random_state=2, n_estimators=97 ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 51, 'n_jobs': None, 'oob_score': False, 'random_state': 2, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "def fit_rf(X, y):\n",
    "    model = RandomForestClassifier(random_state=2)\n",
    "    params = {'n_estimators':list(np.arange(1, 101))}\n",
    "    grid = GridSearchCV(model, params, scoring='recall', cv=3)\n",
    "    grid_fit = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_fit.best_estimator_\n",
    "\n",
    "model_rf = fit_rf(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'n_neighbors'\n",
    "print(model_rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=2, n_estimators=51).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator__bootstrap': True, 'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__max_features': 'auto', 'base_estimator__max_leaf_nodes': None, 'base_estimator__min_impurity_decrease': 0.0, 'base_estimator__min_impurity_split': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'base_estimator__min_weight_fraction_leaf': 0.0, 'base_estimator__n_estimators': 51, 'base_estimator__n_jobs': None, 'base_estimator__oob_score': False, 'base_estimator__random_state': 2, 'base_estimator__verbose': 0, 'base_estimator__warm_start': False, 'base_estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=None,\n",
      "            oob_score=False, random_state=2, verbose=0, warm_start=False), 'learning_rate': 1.0, 'n_estimators': 1, 'random_state': 2}\n"
     ]
    }
   ],
   "source": [
    "def fit_rf_boosted(X, y):\n",
    "    model = AdaBoostClassifier(base_estimator=rf, random_state=2)\n",
    "    params = {'n_estimators':list(np.arange(1, 101))}\n",
    "    grid = GridSearchCV(model, params, scoring='recall', cv=3)\n",
    "    grid_fit = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_fit.best_estimator_\n",
    "\n",
    "model_rf_boosted = fit_rf_boosted(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'n_neighbors'\n",
    "print(model_rf_boosted.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_boosted = AdaBoostClassifier(base_estimator=rf, random_state=2, n_estimators=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator__priors': None, 'base_estimator__var_smoothing': 1e-09, 'base_estimator': GaussianNB(priors=None, var_smoothing=1e-09), 'learning_rate': 1.0, 'n_estimators': 2, 'random_state': 2}\n"
     ]
    }
   ],
   "source": [
    "def fit_nb_boosted(X, y):\n",
    "    model = AdaBoostClassifier(base_estimator=nb, random_state=2)\n",
    "    params = {'n_estimators': list(np.arange(1, 101))}\n",
    "    grid = GridSearchCV(model, params, scoring='recall', cv=3)\n",
    "    grid_fit = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_fit.best_estimator_\n",
    "\n",
    "model_nb_boosted = fit_nb_boosted(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'n_neighbors'\n",
    "print(model_nb_boosted.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_nb = BaggingClassifier(base_estimator=nb, random_state=2, n_estimators=51).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the models using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "CV accuracy: [0.58888889 0.49438202 0.61797753]\n",
      "CV accuracy: 0.567 +/- 0.053\n",
      "Bias Error: 0.43291718684977115\n",
      "Variance Error: 0.00278372730438734\n",
      "\n",
      "Model 2:\n",
      "CV accuracy: [0.64444444 0.56179775 0.52808989]\n",
      "CV accuracy: 0.578 +/- 0.049\n",
      "Bias Error: 0.4218893050353724\n",
      "Variance Error: 0.0023894531890622946\n",
      "\n",
      "Model 3:\n",
      "CV accuracy: [0.65555556 0.53932584 0.56179775]\n",
      "CV accuracy: 0.586 +/- 0.050\n",
      "Bias Error: 0.41444028297960883\n",
      "Variance Error: 0.0025338731634700624\n",
      "\n",
      "Model 4:\n",
      "CV accuracy: [0.65555556 0.53932584 0.59550562]\n",
      "CV accuracy: 0.597 +/- 0.047\n",
      "Bias Error: 0.40320432792342903\n",
      "Variance Error: 0.002252389811667307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "accuracy = []\n",
    "for index, model in enumerate([adaboost_dt, rf, rf_boosted, gbdt]): \n",
    "    print(f'Model {index+1}:')\n",
    "#     kf = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "    scores = cross_val_score(estimator=model,\n",
    "                         X=features,\n",
    "                         y=outcome,\n",
    "                         cv=3,\n",
    "                         n_jobs=1, scoring='recall')\n",
    "    print(f'CV accuracy: {scores}')\n",
    "    print(f'CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n",
    "    print(f'Bias Error: {(np.mean(1 - scores))}')\n",
    "    print(f'Variance Error: {np.var(scores)}')\n",
    "    print()\n",
    "    accuracy.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that using bagged and random forest implementation the variance error reduces with a penalty increase on bias error!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4 has least bias: 0.40320432792342903\n",
      "Model 4 has least variance: 0.002252389811667307\n"
     ]
    }
   ],
   "source": [
    "least_bias_err = 1\n",
    "least_variance_err = 1\n",
    "for index, acc in enumerate(accuracy):\n",
    "    if ((np.mean(1 - acc)) < least_bias_err):\n",
    "        least_bias_err = (np.mean(1 - acc))\n",
    "        index_bias = index\n",
    "    if (np.var(acc) < least_variance_err):\n",
    "        least_variance_err = np.var(acc)\n",
    "        index_var = index\n",
    "print(f'Model {index_bias + 1} has least bias: {least_bias_err}')\n",
    "print(f'Model {index_var + 1} has least variance: {least_variance_err}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therefore for the above data Random forests (Model 3) has the least variance and a slightly lesser bias error as compared to the best.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
